# vllm-sim values

# This values.yaml file creates the resources for random

<<<<<<< HEAD
# When true, LeaderWorkerSet is used instead of Deployment
multinode: false
# When true, an InferencePool is created.
# When false, an InferencePool is not created. Nor is the referencing HTTPRoute.
# Note that when there is no endpointPicker, the InferencePool is not created (overriding a true here).
inferencePool: true
# When true, an HTTPRoute is created.
# When false, an HTTPRoute is not created.
# Note that when inferencePool is false, or there is is no endpointPicker, the HTTPRoute is not created (overriding a true here).
httpRoute: true
=======
multinode: 
  create: false

httpRoute: 
  # When true, an HTTPRoute is created.
  # When false, an HTTPRoute is not created.
  # Note that when inferencePool is false, or there is is no endpointPicker, the HTTPRoute is not created (overriding a true here).
  create: true
>>>>>>> e5681de (enable upstream GAIE charts)

# To control creation of the endpoint picker (inference scheduler), see endpointPicker below

routing:
  # This is the model name for OpenAI requests
  modelName: random
  servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: llm-d-inference-gateway


modelArtifacts:
  uri: "hf://random/modelid"
  size: 5Mi

# describe decode pods
decode:
  create: true
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
prefill:
  create: true
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

<<<<<<< HEAD
# When endpointPicker is present, create the endpoint picker (inference scheduler) objects:
# ServiceAccount, Role, Rolebinding, Deployment, Service
# When endpointPicker is not present, these objects are not created.
# endpointPicker:
#   # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
#   service:
#     # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
#     type: ClusterIP
#     # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
#     port: 9002
#     targetPort: 9002
#     appProtocol: http2
#   debugLevel: 6
#   disableReadinessProbe: true
#   disableLivenessProbe: true
#   replicas: 1
#   # To use a different role from the default, provide the name of the role
#   # permissions: pod-read
=======
endpointPicker:
  create: true
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
  debugLevel: 6
  disableReadinessProbe: true
  disableLivenessProbe: true
  replicas: 1
  role:
    name: default-epp-role
    create: true

inferencePool:
  create: true
>>>>>>> e5681de (enable upstream GAIE charts)
