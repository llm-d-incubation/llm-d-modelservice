# This values.yaml file is a default values file. It can be overridden using another.

modelArtifacts:
  # model URI. One of:
  #   hf://model/name - model on Hugging Face
  #   pvc://pvc_name/path/to/model - model on existing persistant storage
  #   oci:// not yet supported
  uri: "hf://random/modelid"
  # size of volume to create to hold the model
  size: 5Mi
  # name of secret containing credentials for accessing the model (e.g., HF_TOKEN)
  # authSecretName:

# When true, a LeaderWorkerSet is used instead of a Deployment
multinode: false

# Describe routing requirements. In addition to service level routing (OpenAI model name, service port)
# also describes elements for Gateway API Inference Extension configuration
routing:
  # This is the model name for OpenAI requests
  modelName: random/modelId
  # port the VLLM services listen on
  # when a sidecar (proxy) is used it will listen on this port and forward to VLLM on proxy.targetPort
  servicePort: 8000

  # Configuration of VLLM routing sidecar
  # cf. https://github.com/llm-d/llm-d-routing-sidecar/
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    # target port on which VLLM should listen
    targetPort: 8200

  # Reference to parent gateway
  # cf. https://gateway-api.sigs.k8s.io/api-types/gateway/
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway

  # Configuration of InferencePool
  # cf. https://gateway-api-inference-extension.sigs.k8s.io/reference/spec/#inferencepool
  inferencePool:
    create: true
    # name: OVERRIDE_NAME
    # to use a different epp service than the one created when routing.epp.create: true
    # extensionRef:

  inferenceModel:
    create: false

  # Configuration of HTTPRoute (mapping of requests through gateway to InferencePool)
  # cf. https://gateway-api.sigs.k8s.io/api-types/httproute/
  httpRoute:
    create: true
    # when specifiying rules it will overwrite the entire rules block (matches included)
    # rules:
    #   - backendRefs:
    #       - group: inference.networking.x-k8s.io
    #         kind: InferencePool
    #         name: inference-pool-name
    #         port: 8000
    #         weight: 1
    #     matches:
    #       - path:
    #           type: PathPrefix
    #           value: /
    # when specifiying matches and not rules, it will use the default backendRef block but overwrite just the matches section of a single rule
    matches:
      - path:
          type: PathPrefix
          value: /
        # example over-riding matches
        # headers:
        # - name: x-model-name
        #   type: Exact
        #   value: facebook/opt-125m

  # Configuration of EPP (endpoint picker)
  # cf. https://github.com/llm-d/llm-d-inference-scheduler
  epp:
    create: true
    service:
      type: ClusterIP
      port: 9002
      targetPort: 9002
      appProtocol: http2
    image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.4
    replicas: 1
    debugLevel: 4
    disableReadinessProbe: false
    disableLivenessProbe: false
    # To override the name of the inferencepool
    # inferencePool:
    # -- Default environment variables for endpoint picker, use `defaultEnvVarsOverride` to override default behavior by defining the same variable again.
    # Ref: https://github.com/llm-d/llm-d-inference-scheduler/blob/main/docs/architecture.md#scorers--configuration
    env:
    - name: ENABLE_KVCACHE_AWARE_SCORER
      value: "false"
    - name: ENABLE_LOAD_AWARE_SCORER
      value: "true"
    - name: ENABLE_PREFIX_AWARE_SCORER
      value: "true"
    - name: ENABLE_SESSION_AWARE_SCORER
      value: "false"
    - name: KVCACHE_AWARE_SCORER_WEIGHT
      value: "1"
    - name: KVCACHE_INDEXER_REDIS_ADDR
    - name: LOAD_AWARE_SCORER_WEIGHT
      value: "1"
    - name: PD_ENABLED
      value: "false"
    - name: PD_PROMPT_LEN_THRESHOLD
      value: "10"
    - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
      value: "false"
    - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
      value: "false"
    - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
      value: "false"
    - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
      value: "false"
    - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
      value: "1"
    - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
    - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
      value: "1"
    - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
      value: "1"
    - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
      value: "1"
    - name: PREFIX_AWARE_SCORER_WEIGHT
      value: "2"
    - name: SESSION_AWARE_SCORER_WEIGHT
      value: "1"

# Decode pod configuration
decode:
  create: true

  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

# Prefill pod configuation
prefill:
  create: true

  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
