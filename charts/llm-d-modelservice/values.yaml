# This values.yaml file is a default values file. It can be overridden using another.

modelArtifacts:
  # model URI. One of:
  #   hf://model/name - model on Hugging Face
  #   pvc://pvc_name/path/to/model - model on existing persistant storage
  #   oci:// not yet supported
  uri: "hf://random/modelid"
  # size of volume to create to hold the model
  size: 5Mi
  # name of secret containing credentials for accessing the model (e.g., HF_TOKEN)
  # authSecretName:

# When true, a LeaderWorkerSet is used instead of a Deployment
multinode: false

# Describe routing requirements. In addition to service level routing (OpenAI model name, service port)
# also describes elements for Gateway API Inference Extension configuration
routing:
  # This is the model name for OpenAI requests
  modelName: random/modelId
  # port the VLLM services listen on
  # when a sidecar (proxy) is used it will listen on this port and forward to VLLM on proxy.targetPort
  servicePort: 8000

  # Configuration of VLLM routing sidecar
  # cf. https://github.com/llm-d/llm-d-routing-sidecar/
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    # target port on which VLLM should listen
    targetPort: 8200
    # Specify a conenctor to overwrite the default: `nixl`
    connector: nixlv2

    # Boolean: adds the `--secure-proxy` flag to the routingSidecar with your chosen value.  Arg is ommitted by default for compatability with legacy sidecar images.
    secure: true

    # Boolean: whether to use TLS when sending requests to prefillers. Arg is ommitted by default for compatability with legacy sidecar images.
    # prefillerUseTLS: true

    # The path to the certificate for secure proxy.  Arg is ommitted by default for compatability with legacy sidecar images.
    # certPath: "/certs"

    # Overwrite the verbosity of logging in the sidecar (defaults to 5)
    # debugLevel: 5


  # Reference to parent gateway
  # cf. https://gateway-api.sigs.k8s.io/api-types/gateway/
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway

  # Configuration of InferencePool
  # cf. https://gateway-api-inference-extension.sigs.k8s.io/reference/spec/#inferencepool
  inferencePool:
    create: true
    # name: OVERRIDE_NAME
    # to use a different epp service than the one created when routing.epp.create: true
    # extensionRef:

  inferenceModel:
    # Criticality options: ["Critical", "Standard", "Sheddable"], see: https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/config/crd/bases/inference.networking.x-k8s.io_inferencemodels.yaml#L70-L84
    # criticality: Critical
    create: false

  # Configuration of HTTPRoute (mapping of requests through gateway to InferencePool)
  # cf. https://gateway-api.sigs.k8s.io/api-types/httproute/
  httpRoute:
    create: true
    # when specifiying rules it will overwrite the entire rules block (matches included)
    # rules:
    #   - backendRefs:
    #       - group: inference.networking.x-k8s.io
    #         kind: InferencePool
    #         name: inference-pool-name
    #         port: 8000
    #         weight: 1
    #     matches:
    #       - path:
    #           type: PathPrefix
    #           value: /
    # when specifiying matches and not rules, it will use the default backendRef block but overwrite just the matches section of a single rule
    matches:
      - path:
          type: PathPrefix
          value: /
        # example over-riding matches
        # headers:
        # - name: x-model-name
        #   type: Exact
        #   value: facebook/opt-125m

  # Configuration of EPP (endpoint picker)
  # cf. https://github.com/llm-d/llm-d-inference-scheduler
  epp:
    create: true
    service:
      type: ClusterIP
      port: 9002
      targetPort: 9002
      appProtocol: http2
    # image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:v0.5.0-rc.2
    # image: quay.io/grpereir/llm-d-inference-scheduler:v0.5.0-rc.2
    image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.4
    replicas: 1
    debugLevel: 4
    disableReadinessProbe: false
    disableLivenessProbe: false
    # To override the name of the inferencepool
    # inferencePool:
    # -- Default environment variables for endpoint picker, use `defaultEnvVarsOverride` to override default behavior by defining the same variable again.
    # Ref: https://github.com/llm-d/llm-d-inference-scheduler/blob/main/docs/architecture.md#scorers--configuration

    # The name of the plugin file to use. Some default files are provided to you: default-config.yaml, prefix-cache-tracking-config.yaml,
    # prefix-estimate-config.yaml, default-pd-config.yaml, or you may define a custom config below and select it with the pluginsConfigFile field.
    # pluginsConfigFile: "default-plugins.yaml"

    # Adding a custom plugin config via the pluginsCustomConfig field. Inside there should be an entry to a confimap of file containing the `EndpointPickerConfig`
    # pluginsCustomConfig:
    #   custom-plugins.yaml: |
    #     apiVersion: inference.networking.x-k8s.io/v1alpha1
    #     kind: EndpointPickerConfig
    #     plugins:
    #     - type: custom-scorer
    #       parameters:
    #         custom-threshold: 64
    #     - type: max-score-picker
    #     - type: single-profile-handler
    #     schedulingProfiles:
    #     - name: default
    #       plugins:
    #       - pluginRef: custom-scorer
    #         weight: 1
    #       - pluginRef: max-score-picker
    #         weight: 1


    env:
      # Include any Environment variables to epp here, or configure scorers for a legacy epp image, ex:
    # - name: ENABLE_KVCACHE_AWARE_SCORER
    #   value: "false"


# Decode pod configuration
decode:
  create: true
  autoscaling:
    enabled: false

  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

# Prefill pod configuation
prefill:
  create: true
  autoscaling:
    enabled: false

  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
