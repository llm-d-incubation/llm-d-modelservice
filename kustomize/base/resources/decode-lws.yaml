apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: modelservice-decode-lws
  labels:
    llm-d.ai/role: decode
spec:
  replicas: 0  # Disabled by default, multinode component will enable it.
  leaderWorkerTemplate:
    size: 1  # Will be patched by multinode component based on parallelism
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/role: decode
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: random_model
      spec:
        serviceAccountName: modelservice
        containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.6.1
          args: []
          env:
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"
          ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP
          volumeMounts:
            - name: metrics-volume
              mountPath: /.config
            - name: model-storage
              mountPath: /model-cache
          resources:
            limits: {}
            requests: {}
        volumes:
          - name: metrics-volume
            emptyDir: {}
          - name: model-storage
            emptyDir:
              sizeLimit: 5Mi