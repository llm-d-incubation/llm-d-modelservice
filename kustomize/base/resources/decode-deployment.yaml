apiVersion: apps/v1
kind: Deployment
metadata:
  name: modelservice-decode
  labels:
    llm-d.ai/role: decode
spec:
  replicas: 0 # Disabled by default, overlays enable as needed
  selector:
    matchLabels:
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/role: decode
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: random_model
    spec:
      serviceAccountName: modelservice
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d-inference-sim:v0.6.1
        args: []
        env:
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"
        ports:
          - containerPort: 8200
            name: metrics
            protocol: TCP
        volumeMounts:
          - name: metrics-volume
            mountPath: /.config
          - name: model-storage
            mountPath: /model-cache
        resources:
          limits: {}
          requests: {}
      volumes:
        - name: metrics-volume
          emptyDir: {}
        - name: model-storage
          emptyDir:
            sizeLimit: 5Mi