apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: modelservice-prefill-lws
  labels:
    llm-d.ai/role: prefill
spec:
  replicas: 0  # Disabled by default
  leaderWorkerTemplate:
    size: 1  # Will be patched by multinode component based on parallelism
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/role: prefill
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: random_model
      spec:
        serviceAccountName: modelservice
        containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.3.0
          args: []
          env:
            - name: DP_SIZE
              value: "1"
            - name: TP_SIZE
              value: "1"
            - name: DP_SIZE_LOCAL
              value: "1"
          ports:
            - containerPort: 8000
              name: metrics
              protocol: TCP
          volumeMounts:
            - name: metrics-volume
              mountPath: /.config
            - name: model-storage
              mountPath: /model-cache
          resources:
            limits: {}
            requests: {}
        volumes:
          - name: metrics-volume
            emptyDir: {}
          - name: model-storage
            emptyDir:
              sizeLimit: 5Mi