apiVersion: v1
kind: Service
metadata:
  name: decode
  namespace: llm-d
spec:
  selector:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llm-d-xpu-llm-d-modelservice
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000