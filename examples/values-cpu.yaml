# This values.yaml file creates the resources for CPU-only scenario
# Uses a vLLM simulator
# See also defaults in chart values.yaml

# When true, LeaderWorkerSet is used instead of Deployment
multinode: false

modelArtifacts:
  uri: "hf://random/model"
  size: 5Mi

routing:
  # This is the model name for the OpenAI request
  modelName: random/model
  servicePort: 8000

  proxy:
    secure: false

  httpRoute:
    matches:
    - path:
        type: PathPrefix
        value: /
      headers:
      - name: x-model-name
        type: Exact
        value: random/model

  epp:
    create: true
    debugLevel: 6
    disableReadinessProbe: true
    disableLivenessProbe: true

# Decode pod configuation
decode:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault

    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

# Prefill pod configuation
prefill:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault

    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
