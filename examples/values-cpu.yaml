# This values.yaml file creates the resources for CPU-only scenario
# Uses a vLLM simulator
# See also defaults in chart values.yaml

# When true, LeaderWorkerSet is used instead of Deployment
multinode: false

modelArtifacts:
  # name is the value of the model parameter in OpenAI requests
  name: random/model
  uri: "hf://{{ .Values.modelArtifacts.name }}"
  size: 5Mi

routing:
  servicePort: 8000

  proxy:
    secure: false

  httpRoute:
    create: true
    matches:
    - headers:
      - name: x-model-name
        type: Exact
        value: "{{ .Values.modelArtifacts.name }}"

  epp:
    create: true
    debugLevel: 6
    disableReadinessProbe: true
    disableLivenessProbe: true

# Decode pod configuation
decode:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault
    ports:
      - containerPort: 8200  # from routing.proxy.targetPort
        protocol: TCP
    mountModelVolume: true

# Prefill pod configuation
prefill:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault
    ports:
      - containerPort: 8000  # from routing.servicePort
        protocol: TCP
    mountModelVolume: true
