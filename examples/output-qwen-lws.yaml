---
# Source: llm-d-modelservice/templates/epp-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: qwen-lws-llm-d-modelservice-epp
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: qwen-lws-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/epp-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: qwen-lws-llm-d-modelservice-epp
rules:
- apiGroups:
  - inference.networking.x-k8s.io
  resources:
  - inferencemodels
  - inferencepools
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
---
# Source: llm-d-modelservice/templates/epp-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: qwen-lws-llm-d-modelservice-epp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: qwen-lws-llm-d-modelservice-epp
subjects:
- kind: ServiceAccount
  name: qwen-lws-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/epp-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: qwen-lws-llm-d-modelservice-epp
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9002
      targetPort: 9002
      protocol: TCP
      appProtocol: http2
  selector:
    llm-d.ai/epp: qwen-lws-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/epp-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-lws-llm-d-modelservice-epp
  labels:
    llm-d.ai/epp: qwen-lws-llm-d-modelservice-epp
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/epp: qwen-lws-llm-d-modelservice-epp
  template:
    metadata:
      labels:
        llm-d.ai/epp: qwen-lws-llm-d-modelservice-epp
    spec:
      containers:
      - name: epp
        imagePullPolicy: Always
        image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
        args:
        - --poolName
        - qwen-lws-llm-d-modelservice
        - --poolNamespace
        - default
        - -v
        - "5"
        - --zap-encoder
        - json
        - --grpcPort
        - "9002"
        - --grpcHealthPort
        - "9003"
        env:
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: ENABLE_LOAD_AWARE_SCORER
          value: "true"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: KVCACHE_AWARE_SCORER_WEIGHT
          value: "1"
        - name: KVCACHE_INDEXER_REDIS_ADDR
        - name: LOAD_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PD_ENABLED
          value: "false"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "10"
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
        - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFIX_AWARE_SCORER_WEIGHT
          value: "2"
        - name: SESSION_AWARE_SCORER_WEIGHT
          value: "1"
        ports:
        - containerPort: 9002
          name: grpc
          protocol: TCP
        - containerPort: 9003
          name: grpc-health
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        readinessProbe:
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
      serviceAccount: qwen-lws-llm-d-modelservice-epp
      serviceAccountName: qwen-lws-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: qwen-lws-llm-d-modelservice
  namespace: default
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway
  rules:
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: qwen-lws-llm-d-modelservice
      port: 8080
      weight: 1
    matches:
    - path:
        type: PathPrefix
        value: /
---
# Source: llm-d-modelservice/templates/inferencepool.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: qwen-lws-llm-d-modelservice
  namespace: default
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: qwen-lws-llm-d-modelservice-epp
  selector:
    leaderworkerset.sigs.k8s.io/worker-index: "0"
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
  targetPortNumber: 8080
---
# Source: llm-d-modelservice/templates/decode-lws.yaml
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: qwen-lws-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
    llm-d.ai/role: decode
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    leaderTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: qwen-lws-llm-d-modelservice
          llm-d.ai/role: decode
      spec:

        initContainers:
          - name: routing-proxy
            args:
              - --port=8080
              - --vllm-port=8200
              - --connector=nixlv2
              - -v=5
            image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
            imagePullPolicy: Always
            ports:
              - containerPort: 8080
            resources: {}
            restartPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
      
        serviceAccountName: qwen-lws-llm-d-modelservice
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: gpu.nvidia.com/model
                    operator: In
                    values:
                      - H200
        volumes:
          - configMap:
              defaultMode: 493
              name: vllm-init-scripts-config
            name: init-scripts-volume
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - name: vllm
            persistentVolumeClaim:
              claimName: tms-vllm
          - name: model-storage
            persistentVolumeClaim:
              claimName: tms-hf-cache
              readOnly: true
        containers:
        - name: vllm-worker
          image: quay.io/tms/vllm-dev-pplx:0.1.0
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
          imagePullPolicy: Always

          command:
            - /bin/sh
            - -c
          args:
            - |
              #################
              # Interactive section
              #################
              # export VENV_PATH="/code/venv"
              # source /init-scripts/common.sh
              # /init-scripts/dotfiles.sh
              # sleep infinity
              #################
              #
              #################
              # Install vLLM
              #################
              VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
              #################
              # RUN vLLM
              #################
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
              if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                #################
                # Leader-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8200 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code
              else
                #################
                # Worker-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8200 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code \
                  --headless
              fi
          env:
          - name: VLLM_TORCH_PROFILER_DIR
            value: /code/traces
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_REPO_URL
            value: https://github.com/tlrmchlsmth/vllm.git
          - name: VLLM_BRANCH
            value: for_michael_k
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: pplx
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_DEBUG
            value: INFO
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: GH_TOKEN_FROM_SECRET
            valueFrom:
              secretKeyRef:
                key: GH_TOKEN
                name: gh-token-secret
                optional: true
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "6555"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: DP_SIZE
            value: "4"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "2"
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-secret
                key: HF_TOKEN

          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
            requests:
              cpu: 32
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /init-scripts
            name: init-scripts-volume
          - mountPath: /code
            name: vllm
          - name: model-storage
            mountPath: /model-cache
          workingDir: /code
          stdin: true
          tty: true

    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: qwen-lws-llm-d-modelservice
          llm-d.ai/role: decode
      spec:
      
        serviceAccountName: qwen-lws-llm-d-modelservice
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: gpu.nvidia.com/model
                    operator: In
                    values:
                      - H200
        volumes:
          - configMap:
              defaultMode: 493
              name: vllm-init-scripts-config
            name: init-scripts-volume
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - name: vllm
            persistentVolumeClaim:
              claimName: tms-vllm
          - name: model-storage
            persistentVolumeClaim:
              claimName: tms-hf-cache
              readOnly: true
        containers:
        - name: vllm-worker
          image: quay.io/tms/vllm-dev-pplx:0.1.0
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
          imagePullPolicy: Always

          command:
            - /bin/sh
            - -c
          args:
            - |
              #################
              # Interactive section
              #################
              # export VENV_PATH="/code/venv"
              # source /init-scripts/common.sh
              # /init-scripts/dotfiles.sh
              # sleep infinity
              #################
              #
              #################
              # Install vLLM
              #################
              VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
              #################
              # RUN vLLM
              #################
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
              if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                #################
                # Leader-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8200 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code
              else
                #################
                # Worker-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8200 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code \
                  --headless
              fi
          env:
          - name: VLLM_TORCH_PROFILER_DIR
            value: /code/traces
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_REPO_URL
            value: https://github.com/tlrmchlsmth/vllm.git
          - name: VLLM_BRANCH
            value: for_michael_k
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: pplx
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_DEBUG
            value: INFO
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: GH_TOKEN_FROM_SECRET
            valueFrom:
              secretKeyRef:
                key: GH_TOKEN
                name: gh-token-secret
                optional: true
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "6555"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: DP_SIZE
            value: "4"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "2"
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-secret
                key: HF_TOKEN

          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
            requests:
              cpu: 32
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /init-scripts
            name: init-scripts-volume
          - mountPath: /code
            name: vllm
          - name: model-storage
            mountPath: /model-cache
          workingDir: /code
          stdin: true
          tty: true
---
# Source: llm-d-modelservice/templates/prefill-lws.yaml
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: qwen-lws-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.8
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
    llm-d.ai/role: prefill
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2

    # no sidecar so no need to specify leader separately

    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: qwen-lws-llm-d-modelservice
          llm-d.ai/role: prefill
      spec:
      
        serviceAccountName: qwen-lws-llm-d-modelservice
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: gpu.nvidia.com/model
                    operator: In
                    values:
                      - H200
        volumes:
          - configMap:
              defaultMode: 493
              name: vllm-init-scripts-config
            name: init-scripts-volume
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - name: vllm
            persistentVolumeClaim:
              claimName: tms-vllm
          - name: model-storage
            persistentVolumeClaim:
              claimName: tms-hf-cache
              readOnly: true
        containers:
        - name: vllm-worker
          image: quay.io/tms/vllm-dev-pplx:0.1.0
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
          imagePullPolicy: Always

          command:
            - /bin/sh
            - -c
          args:
            - |
              #################
              # Interactive section
              #################
              # export VENV_PATH="/code/venv"
              # source /init-scripts/common.sh
              # /init-scripts/dotfiles.sh
              # sleep infinity
              #################
              #
              #################
              # Install vLLM
              #################
              VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
              #################
              # RUN vLLM
              #################
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
              if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                #################
                # Leader-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8080 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code
              else
                #################
                # Worker-only launch
                #################
                exec /app/venv/bin/vllm serve \
                  Qwen/Qwen3-30B-A3B-FP8 \
                  --port 8080 \
                  --disable-log-requests \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --trust-remote-code \
                  --headless
              fi
          env:
          - name: VLLM_TORCH_PROFILER_DIR
            value: /code/traces
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_REPO_URL
            value: https://github.com/tlrmchlsmth/vllm.git
          - name: VLLM_BRANCH
            value: for_michael_k
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: pplx
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_DEBUG
            value: INFO
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: GH_TOKEN_FROM_SECRET
            valueFrom:
              secretKeyRef:
                key: GH_TOKEN
                name: gh-token-secret
                optional: true
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "6555"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: DP_SIZE
            value: "4"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "2"
          - name: HF_HOME
            value: /model-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-secret
                key: HF_TOKEN

          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
            requests:
              cpu: 32
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: 2
              rdma/ib: 1
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /init-scripts
            name: init-scripts-volume
          - mountPath: /code
            name: vllm
          - name: model-storage
            mountPath: /model-cache
          workingDir: /code
          stdin: true
          tty: true
