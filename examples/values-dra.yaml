# This example demonstrates the following capabilities:
# 1. Use of Dynamic Resource Allocation (DRA) with the new accelerator.dra API
# 2. Via DRA, select Intel Gaudi devices for the decode pod
# 3. Auto-calculation of device count from parallelism settings
# 4. For clarity, all other features are disabled, only Deployment and ResourceClaimTemplate are created

modelArtifacts:
  name: random/model
  uri: "hf://{{ .Values.modelArtifacts.name }}"
  size: 50Gi
  authSecretName: "llm-d-hf-token"
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: random_model

# New DRA API: Enable DRA for accelerators
accelerator:
  type: intel-gaudi
  dra: true
  # The resourceClaimTemplate for intel-gaudi is defined in values.yaml
  # Count will be auto-calculated from decode.parallelism (tensor * dataLocal)
  # To override, uncomment and set:
  # resourceClaimTemplates:
  #   intel-gaudi:
  #     name: intel-gaudi-claim-template
  #     class: gaudi.intel.com
  #     match: "exactly"
  #     count: 2  # Explicit count override
  #     selectors: []

# Routing configuration
routing:
  proxy:
    enabled: false

decode:
  create: true
  # Parallelism settings determine the auto-calculated device count
  parallelism:
    tensor: 2      # Will request 2 Gaudi devices (2 * 1 = 2)
    dataLocal: 1
  containers:
  - name: vllm
    image: intel/vllm:0.10.0-xpu
    modelCommand: "vllmServe"
    args:
    - --dtype=float16
    - --block-size=64
    - --max-num-seqs=64
    - --max-model-len=2048
    - --max-num-batched-token=4096
    - --disable-sliding-window
    - --gpu-memory-util=0.9
    - --quantization=fp8
    env:
    - name: OMPI_MCA_btl_vader_single_copy_mechanism
      value: none
    - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
      value: "true"
    - name: VLLM_SKIP_WARMUP
      value: "true"
    mountModelVolume: true
    # When using DRA (accelerator.dra: true) -- as in this example:
    # - Do NOT specify accelerator resources in limits (e.g., habana.ai/gaudi)
    # - Accelerator allocation is handled via claims (auto-added by the chart)
    # - You CAN specify non-accelerator resources like CPU and memory
    # - You CAN specify other resources via claims (as in this example):
    resources:
      limits:
        cpu: "4"
        memory: "16Gi"
      requests:
        cpu: "2"
        memory: "8Gi"
  resourceClaims:
  - name: llm-d-existing-claim-sample
    resourceClaimTemplateName: llm-d-existing-claim-sample

prefill:
  create: false
multinode: false
