# Example values for Intel XPU with Prefill/Decode disaggregation
# WARNING: Intel XPU does not currently support P/D disaggregation
# This configuration is for reference only - testing shows:
# - SimpleConnector: Not supported
# - NixlConnector: Requires NIXL library which is not available
# Use values-xpu.yaml for working Intel XPU deployment instead

modelArtifacts:
  name: microsoft/DialoGPT-large
  uri: "hf://microsoft/DialoGPT-large"
  size: 10Gi

accelerator:
  type: "intel-i915"

routing:
  servicePort: 8000
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0
    targetPort: 8200
    # Use compatible connector for XPU
    connector: nixlv2

  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway
    namespace: "{{ .Release.Namespace }}"

  httpRoute:
    create: true
    matches:
      - path:
          type: PathPrefix
          value: /
        headers:
        - name: x-model-name
          type: Exact
          value: "{{ .Values.modelArtifacts.name }}"

  epp:
    create: true
    image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.2.1
    # Use XPU-compatible configuration
    pluginsConfigFile: "default-config.yaml"

# Decode pod configuration for Intel XPU
decode:
  create: true
  replicas: 1  # Just 1 decode pod
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-xpu:v0.2.0"
    imagePullPolicy: Never
    modelCommand: custom
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    args:
      - "--model"
      - "microsoft/DialoGPT-large"
      - "--enforce-eager"
      - "--tensor-parallel-size"
      - "1"  # TP=1 for each decode pod (using 1 XPU each)
      - "--port"
      - "8200"
      - "--host"
      - "0.0.0.0"
      - "--kv-transfer-config"
      - '{"kv_connector":"NixlConnector", "kv_role":"kv_consumer"}'
    env:
      - name: ZE_AFFINITY_MASK
        value: "0"  # Decode pod uses XPU 0
      - name: ZE_ENABLE_PCI_ID_DEVICE_ORDER
        value: "1"
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
        value: "5557"
      - name: VLLM_LOGGING_LEVEL
        value: DEBUG
      # Intel XPU specific environment variables
      - name: TORCH_LLM_ALLREDUCE
        value: "1"
      - name: VLLM_USE_V1
        value: "1"
      - name: CCL_ZE_IPC_EXCHANGE
        value: "pidfd"
      - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
        value: "1"
      - name: VLLM_WORKER_MULTIPROC_METHOD
        value: "spawn"
    ports:
      - containerPort: 8200
        protocol: TCP
      - containerPort: 5557  # NIXL side channel
        protocol: TCP
    resources:
      limits:
        memory: 24Gi  # Reduced memory since each pod uses 1 XPU
        cpu: "8"      # Reduced CPU
        gpu.intel.com/i915: "1"  # Each decode pod uses 1 XPU
      requests:
        cpu: "4"
        memory: 12Gi
        gpu.intel.com/i915: "1"
    mountModelVolume: true

  acceleratorTypes:
    labelKey: "accelerator"
    labelValues:
      - "intel-xpu"

# Prefill pod configuration for Intel XPU
prefill:
  create: true
  replicas: 1  # Just 1 prefill pod
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-xpu:v0.2.0"
    modelCommand: custom
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    args:
      - "--model"
      - "microsoft/DialoGPT-large"
      - "--enforce-eager"
      - "--tensor-parallel-size"
      - "1"  # TP=1 for prefill
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "--kv-transfer-config"
      - '{"kv_connector":"NixlConnector", "kv_role":"kv_producer"}'
    env:
      - name: ZE_AFFINITY_MASK
        value: "1"  # Prefill pod uses XPU 1
      - name: ZE_ENABLE_PCI_ID_DEVICE_ORDER
        value: "1"
      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
        value: "5557"
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: VLLM_LOGGING_LEVEL
        value: DEBUG
      # Intel XPU specific environment variables
      - name: TORCH_LLM_ALLREDUCE
        value: "1"
      - name: VLLM_USE_V1
        value: "1"
      - name: CCL_ZE_IPC_EXCHANGE
        value: "pidfd"
      - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
        value: "1"
      - name: VLLM_WORKER_MULTIPROC_METHOD
        value: "spawn"
    ports:
      - containerPort: 8000
        protocol: TCP
      - containerPort: 5557
        protocol: TCP
    resources:
      limits:
        memory: 32Gi
        cpu: "16"
        gpu.intel.com/i915: "1"
      requests:
        cpu: "8"
        memory: 16Gi
        gpu.intel.com/i915: "1"
    mountModelVolume: true

  acceleratorTypes:
    labelKey: "accelerator"
    labelValues:
      - "intel-xpu"

multinode: false
