---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: qwen-lws-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.11
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: qwen-lws-llm-d-modelservice
  namespace: default
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.11
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: infra-wide-pd-inference-gateway
  rules:
    - backendRefs:
      - group: inference.networking.x-k8s.io
        kind: InferencePool
        name: gaie-pd
        port: 8000
        weight: 1
      matches:
      - path:
          type: PathPrefix
          value: /
---
# Source: llm-d-modelservice/templates/inferencemodel.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
  name: qwen-lws-llm-d-modelservice
spec:
  modelName: Qwen/Qwen3-30B-A3B-FP8
  poolRef:
    name: gaie-pd
---
# Source: llm-d-modelservice/templates/decode-lws.yaml
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: qwen-lws-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.11
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
    llm-d.ai/role: decode
  annotations:
    leaderworkerset.sigs.k8s.io/subgroup-exclusive-topology: kubernetes.io/hostname
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 8
    subGroupPolicy:
      subGroupSize: 8
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: qwen-lws-llm-d-modelservice
          llm-d.ai/role: decode
      spec:
        hostIPC: true
        hostPID: true
        
        initContainers:
          - name: routing-proxy
            args:
              - --port=8000
              - --vllm-port=8200
              - --connector=nixlv2
              - -v=5
            image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
            imagePullPolicy: Always
            ports:
              - containerPort: 8000
            resources: {}
            restartPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
      
        serviceAccountName: qwen-lws-llm-d-modelservice
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: gpu.nvidia.com/model
                    operator: In
                    values:
                      - H200
        volumes:
          - name: model-storage
            emptyDir: 
              sizeLimit: 100Gi
        containers:
        - name: vllm-worker-decode
          image: quay.io/tms/vllm-dev-deepep:0.1.0
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
            privileged: true
          imagePullPolicy: Always
          
          command:
            - /bin/bash
            - -c
          args:
            - |
              ###################################################
              # Figure out which GPU to use when 'privileged: true'
              # vLLM requires that CPU_VISIBLE_DEVICES is set to device ordinals,
              # so we need to remap the GPU UUIDs to indices.
            
              echo "NVIDIA_VISIBLE_DEVICES=$NVIDIA_VISIBLE_DEVICES"
              ALLOCATED_GPU_UUID=$(ls "$NVIDIA_VISIBLE_DEVICES" | head -n 1)
              echo "Allocated GPU UUID:  $ALLOCATED_GPU_UUID"
              GPU_MAP_DATA=$(nvidia-smi --query-gpu=uuid,index --format=csv,noheader)
              echo "nvidia-smi output for UUIDs and indices:"
              echo "$GPU_MAP_DATA"
            
              export ALLOCATED_GPU_INDEX=$(
                echo "$GPU_MAP_DATA" |
                awk -F',' -v target_uuid="$ALLOCATED_GPU_UUID" '
                  {
                    gsub(/^[ \t]+|[ \t]+$/, "", $1)
                    if ($1 == target_uuid) {
                      gsub(/^[ \t]+|[ \t]+$/, "", $2)
                      print $2
                      exit
                    }
                  }'
              )
            
              echo "Using GPU Device ID: $ALLOCATED_GPU_INDEX"
              #################
              # RUN vLLM
              #################
              export CUDA_VISIBLE_DEVICES=${ALLOCATED_GPU_INDEX}
            
              vllm serve \
                Qwen/Qwen3-30B-A3B-FP8 \
                --port 8200 \
                --disable-log-requests \
                --enable-expert-parallel \
                --tensor-parallel-size $TP_SIZE \
                --data-parallel-size $(LWS_GROUP_SIZE) \
                --data-parallel-address $(LWS_LEADER_ADDRESS) \
                --data-parallel-rpc-port 5555 \
                --data-parallel-rank $(LWS_WORKER_INDEX) \
                --trust-remote-code  \
                --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
          env:
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: naive
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: HF_HUB_CACHE
            value: /huggingface-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                key: HF_TOKEN
                name: llm-d-hf-token
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: DP_SIZE
            value: "8"
          - name: TP_SIZE
            value: "1"
          
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          
          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 128Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
            requests:
              cpu: 8
              ephemeral-storage: 64Gi
              memory: 128Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
          
          workingDir: /code
---
# Source: llm-d-modelservice/templates/prefill-lws.yaml
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: qwen-lws-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.11
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: qwen-lws-llm-d-modelservice
    llm-d.ai/role: prefill
  annotations:
    leaderworkerset.sigs.k8s.io/subgroup-exclusive-topology: kubernetes.io/hostname
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 8
    subGroupPolicy:
      subGroupSize: 8

    # no sidecar so no need to specify leader separately

    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: qwen-lws-llm-d-modelservice
          llm-d.ai/role: prefill
      spec:
        hostIPC: true
        hostPID: true
      
        serviceAccountName: qwen-lws-llm-d-modelservice
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: gpu.nvidia.com/model
                    operator: In
                    values:
                      - H200
        volumes:
          - name: model-storage
            emptyDir: 
              sizeLimit: 100Gi
        containers:
        - name: vllm-worker-prefill
          image: quay.io/tms/vllm-dev-deepep:0.1.0
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
            privileged: true
          imagePullPolicy: Always
          
          command:
            - /bin/bash
            - -c
          args:
            - |
              ###################################################
              # Figure out which GPU to use when 'privileged: true'
              # vLLM requires that CPU_VISIBLE_DEVICES is set to device ordinals,
              # so we need to remap the GPU UUIDs to indices.
            
              echo "NVIDIA_VISIBLE_DEVICES=$NVIDIA_VISIBLE_DEVICES"
              ALLOCATED_GPU_UUID=$(ls "$NVIDIA_VISIBLE_DEVICES" | head -n 1)
              echo "Allocated GPU UUID:  $ALLOCATED_GPU_UUID"
              GPU_MAP_DATA=$(nvidia-smi --query-gpu=uuid,index --format=csv,noheader)
              echo "nvidia-smi output for UUIDs and indices:"
              echo "$GPU_MAP_DATA"
            
              export ALLOCATED_GPU_INDEX=$(
                echo "$GPU_MAP_DATA" |
                awk -F',' -v target_uuid="$ALLOCATED_GPU_UUID" '
                  {
                    gsub(/^[ \t]+|[ \t]+$/, "", $1)
                    if ($1 == target_uuid) {
                      gsub(/^[ \t]+|[ \t]+$/, "", $2)
                      print $2
                      exit
                    }
                  }'
              )
            
              echo "Using GPU Device ID: $ALLOCATED_GPU_INDEX"
              #################
              # RUN vLLM
              #################
              export CUDA_VISIBLE_DEVICES=${ALLOCATED_GPU_INDEX}
            
              vllm serve \
                Qwen/Qwen3-30B-A3B-FP8 \
                --port 8080 \
                --disable-log-requests \
                --enable-expert-parallel \
                --tensor-parallel-size $TP_SIZE \
                --data-parallel-size $(LWS_GROUP_SIZE) \
                --data-parallel-address $(LWS_LEADER_ADDRESS) \
                --data-parallel-rpc-port 5555 \
                --data-parallel-rank $(LWS_WORKER_INDEX) \
                --trust-remote-code \
                --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
          env:
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: naive
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: HF_HUB_CACHE
            value: /huggingface-cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                key: HF_TOKEN
                name: llm-d-hf-token
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: DP_SIZE
            value: "8"
          - name: TP_SIZE
            value: "1"
          
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          
          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 128Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
            requests:
              cpu: 8
              ephemeral-storage: 64Gi
              memory: 128Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
          
          workingDir: /code
